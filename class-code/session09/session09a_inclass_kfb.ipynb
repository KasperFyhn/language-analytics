{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6701f8-424e-4dbb-a208-59f232970940",
   "metadata": {},
   "source": [
    "# Working with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5374aa-257e-4ec3-add4-cbe26f926350",
   "metadata": {},
   "source": [
    "So far we've seen a couple of key Python libraries for doing specific tasks in NLP. For example, ```scikit-learn``` provides a whole host of fundamental machine learning algortithms; ```spaCy``` allows us to do robust linguistic analysis; ```pandas``` for working with dataframes; ```numpy``` for numerical calculations; and ```matplotlib``` for simple plotting.\n",
    "\n",
    "Today, we're going to meet ```gensim``` which is the best way to work with (static) word embeddings like word2vec. You can find the documentation [here](https://radimrehurek.com/gensim/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377bfa0c-a4fc-4b31-9e20-aeba254db6d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:15:45.479574Z",
     "start_time": "2025-03-31T08:15:43.750845Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:12.530529Z",
     "iopub.status.busy": "2022-10-13T11:26:12.529804Z",
     "iopub.status.idle": "2022-10-13T11:26:13.556862Z",
     "shell.execute_reply": "2022-10-13T11:26:13.555130Z",
     "shell.execute_reply.started": "2022-10-13T11:26:12.530471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "import gensim.downloader as api\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11e678-a148-42f4-983d-e71e2a5fa9ab",
   "metadata": {},
   "source": [
    "Gensim has many pre-trained static word embeddings. Below, I get a list via the gensim download API to see the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8eb52fb9663aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:15:51.111585Z",
     "start_time": "2025-03-31T08:15:45.493138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "\t description : 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n",
      "\t base_dataset : Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)\n",
      "conceptnet-numberbatch-17-06-300\n",
      "\t description : ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\n",
      "\t base_dataset : ConceptNet, word2vec, GloVe, and OpenSubtitles 2016\n",
      "word2vec-ruscorpora-300\n",
      "\t description : Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\n",
      "\t base_dataset : Russian National Corpus (about 250M words)\n",
      "word2vec-google-news-300\n",
      "\t description : Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\n",
      "\t base_dataset : Google News (about 100 billion words)\n",
      "glove-wiki-gigaword-50\n",
      "\t description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n",
      "glove-wiki-gigaword-100\n",
      "\t description : Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n",
      "glove-wiki-gigaword-200\n",
      "\t description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n",
      "glove-wiki-gigaword-300\n",
      "\t description : Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n",
      "glove-twitter-25\n",
      "\t description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\n",
      "glove-twitter-50\n",
      "\t description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
      "\t base_dataset : Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\n",
      "glove-twitter-100\n",
      "\t description : Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
      "\t base_dataset : Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\n",
      "glove-twitter-200\n",
      "\t description : Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\n",
      "\t base_dataset : Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\n",
      "__testing_word2vec-matrix-synopsis\n",
      "\t description : [THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\n"
     ]
    }
   ],
   "source": [
    "# keys in the returned dicts that are of interest to use\n",
    "info_keys = ['description', 'base_dataset']\n",
    "\n",
    "\n",
    "for model_name, model_info in gensim.downloader.info()['models'].items():\n",
    "    print(model_name)\n",
    "    for key in info_keys:\n",
    "        if key not in model_info:\n",
    "            continue\n",
    "        info = model_info[key]\n",
    "        print('\\t', key, ':', info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed41b0a-a40c-4bc5-b98a-e87d79c05d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:16:41.055274Z",
     "start_time": "2025-03-31T08:15:51.304101Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:15.090456Z",
     "iopub.status.busy": "2022-10-13T11:26:15.089708Z",
     "iopub.status.idle": "2022-10-13T11:26:17.622369Z",
     "shell.execute_reply": "2022-10-13T11:26:17.620582Z",
     "shell.execute_reply.started": "2022-10-13T11:26:15.090393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# go ahead an experiment with different models.\n",
    "# below I will use a smaller model for demo purposes\n",
    "# bigger models will take some time to load!\n",
    "model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf79327-ffe5-43ba-8f09-3ee8e4ec3c95",
   "metadata": {},
   "source": [
    "I've (not I Kasper, but Ross. Thanks, Ross!) outlined a couple of tasks for you below to experiment with. Use these just a stepping off points to explore the nature of word embeddings and how they work.\n",
    "\n",
    "Work in small groups on these tasks and make sure to discuss the issues and compare results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0777d6e2662b",
   "metadata": {},
   "source": [
    "### Task 0: The vectors\n",
    "\n",
    "Have a look at a vector. Maybe two. While they are black box representations of words, it may help you to get a sense of what they are in practice: long lists of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30724dd9345f7c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:18:07.686274Z",
     "start_time": "2025-03-31T08:18:07.681925Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_vector = model[\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb8c15a8342f087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:18:08.548397Z",
     "start_time": "2025-03-31T08:18:08.543044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7a042328785145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:20:27.884153Z",
     "start_time": "2025-03-31T08:20:27.876223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28378   ,  0.51154006,  0.58068997,  0.06518   , -0.316148  ,\n",
       "        0.09908   ,  0.68097997,  0.32469   ,  0.46798003,  0.45884332,\n",
       "       -0.18392   ,  0.74072003, -2.62524   ,  0.44317302, -0.184625  ,\n",
       "        0.55441004,  0.07789001,  0.281647  ,  0.152997  , -0.83138   ,\n",
       "       -0.2303432 , -0.03797999,  0.5052    , -0.5836081 ,  0.81937003,\n",
       "       -1.52746   , -0.67188   ,  0.05814001,  0.52458   ,  0.17989   ,\n",
       "        0.253675  ,  0.05417   ,  0.07034999, -0.159674  ,  0.98205996,\n",
       "       -0.28252   , -0.637431  , -0.237894  ,  0.42475998,  0.4218149 ,\n",
       "       -0.91656   ,  0.18882   , -0.97351   , -0.32827199,  0.0914    ,\n",
       "       -0.04236001,  0.5993966 ,  0.35039002, -0.23984   , -0.538397  ,\n",
       "       -0.589912  ,  0.11551   ,  0.36722898, -0.30347002,  0.5567    ,\n",
       "        0.02548999,  0.25160003,  0.24816   ,  0.90613997,  0.076362  ,\n",
       "       -0.201481  ,  0.04405999,  0.89585   , -0.69548   , -0.40938997,\n",
       "       -0.75379   , -0.836972  ,  0.27672   ,  0.05488998,  0.07525   ,\n",
       "       -0.00566   , -0.21309601,  0.26648998, -0.90571004, -0.06518999,\n",
       "       -0.485611  , -0.57111   ,  0.03542   , -0.54859996,  0.25792003,\n",
       "        0.62279004,  0.04562998, -0.21667   , -0.34952003, -0.11413001,\n",
       "       -0.37755996,  0.03572001,  0.553273  , -0.144887  ,  0.11471   ,\n",
       "       -0.24133499,  0.04716999, -0.5388    , -0.29250997,  0.11058   ,\n",
       "        0.204617  ,  0.23739001, -0.08699   , -0.03126001, -0.46212   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When they arre nothing but regular vectors in practice, we can do all sorts of things with them. Some may make sense, some may not.\n",
    "\n",
    "# n principle, this may remove the \"feline\" part of a cat. What might be left?\n",
    "model[\"cat\"] - model[\"feline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560e004440426635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:22:05.867468Z",
     "start_time": "2025-03-31T08:22:05.863767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.606619"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dot product\n",
    "np.dot(model[\"cat\"], model[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bc9637a6b0a5af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:24:33.379800Z",
     "start_time": "2025-03-31T08:24:33.373758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.664348"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of a vector, spelled out\n",
    "np.sqrt(np.dot(model[\"cat\"], model[\"cat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d604e11-9b07-4d12-b10f-3309484819fa",
   "metadata": {},
   "source": [
    "### Task 1: Finding polysemy\n",
    "\n",
    "Find a polysemous word (i.e. words with more than one meaning) such that the top-10 most similar words (according to cosine similarity) contains related words from both meanings. An example is given for you below in English.\n",
    "\n",
    "Are there certain words for which polysemy is more of a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd074b4-23ee-4d70-afc8-db85bd53e904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:28:14.943223Z",
     "start_time": "2025-03-25T07:28:14.901319Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:26:39.457702Z",
     "iopub.status.busy": "2022-10-13T11:26:39.457000Z",
     "iopub.status.idle": "2022-10-13T11:26:39.620121Z",
     "shell.execute_reply": "2022-10-13T11:26:39.618193Z",
     "shell.execute_reply.started": "2022-10-13T11:26:39.457645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mickey', 0.7609332799911499),\n",
       " ('rabbit', 0.6879944801330566),\n",
       " ('hamster', 0.6657567620277405),\n",
       " ('cat', 0.6595972180366516),\n",
       " ('minnie', 0.6593716740608215),\n",
       " ('keyboard', 0.6403798460960388),\n",
       " ('turtle', 0.6392719149589539),\n",
       " ('kitty', 0.6229013800621033),\n",
       " ('camera', 0.6178072690963745),\n",
       " ('dog', 0.6132174134254456)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"mouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7ebd60cabbebe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:30:21.488563Z",
     "start_time": "2025-03-25T07:30:21.485949Z"
    }
   },
   "outputs": [],
   "source": [
    "# put other examples here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78f14e-45b0-4538-ae54-ffeb01836618",
   "metadata": {},
   "source": [
    "### Task 2: Synonyms and antonyms\n",
    "\n",
    "In the lecture, we saw that _cosine similarity_ can also be thought of as _cosine distance_, which is simply ```1 - cosine similarity```. So the higher the cosine distance, the further away two words are from each other and so they have less \"in common\".\n",
    "\n",
    "Find three words ```(w1,w2,w3)``` where ```w1``` and ```w2``` are synonyms (same meaning) and ```w1``` and ```w3``` are antonyms (opposite meaning), but where: \n",
    "\n",
    "```Cosine Distance(w1,w3) < Cosine Distance(w1,w2)```\n",
    "\n",
    "For example, if we look at the example below:\n",
    "\n",
    "w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
    "\n",
    "Once you have found your example, think about a possible explanation for why this counter-intuitive result may have happened. Are there any inconsistencies?\n",
    "\n",
    "You should use the the ```model.distance(w1, w2)``` function here in order to compute the cosine distance between two words. I've given a starting example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b476719-0cc1-4da2-bdd9-98bd22bd48e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:45.510050Z",
     "start_time": "2025-03-20T12:42:45.505091Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:28:51.002030Z",
     "iopub.status.busy": "2022-10-13T11:28:51.001364Z",
     "iopub.status.idle": "2022-10-13T11:28:51.012787Z",
     "shell.execute_reply": "2022-10-13T11:28:51.011701Z",
     "shell.execute_reply.started": "2022-10-13T11:28:51.001973Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4100914001464844"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\", \"sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72d9efe2-809f-4944-9e6d-a9c735ccc4ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:45.913253Z",
     "start_time": "2025-03-20T12:42:45.909405Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:28:36.667432Z",
     "iopub.status.busy": "2022-10-13T11:28:36.666728Z",
     "iopub.status.idle": "2022-10-13T11:28:36.677467Z",
     "shell.execute_reply": "2022-10-13T11:28:36.676303Z",
     "shell.execute_reply.started": "2022-10-13T11:28:36.667375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339455842971802"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\",\"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ed87ba7-851c-42bb-91a1-0033c6ebaa79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:46.348978Z",
     "start_time": "2025-03-20T12:42:46.344945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"happy\", \"sad\") < model.distance(\"happy\",\"cheerful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cda1045c6f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86210994-55e0-4ef1-99c8-9345e297cbc9",
   "metadata": {},
   "source": [
    "### Task 3: Word analogies\n",
    "\n",
    "We saw in the lecture that we can use basic arithmetic on word embeddings, in order to conduct word analogy task.\n",
    "\n",
    "For example:\n",
    "\n",
    "```man::king as woman::queen```\n",
    "\n",
    "So we can say that if we take the vector for ```king``` and subtract the vector for ```man```, we're removing the gender component from the ```king```. If we then add ```woman``` to the resulting vector, we should be left with a vector similar to ```queen```.\n",
    "\n",
    "NB: It might not be _exactly_ the vector for ```queen```, but it should at least be _close_ to it.\n",
    "\n",
    "```gensim``` has some quirky syntax that allows us to perform this kind of arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e97f38cc-a6a6-4d54-a0ef-97b90bef4140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:48.457746Z",
     "start_time": "2025-03-20T12:42:48.426311Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:37:01.747364Z",
     "iopub.status.busy": "2022-10-13T11:37:01.746670Z",
     "iopub.status.idle": "2022-10-13T11:37:01.859900Z",
     "shell.execute_reply": "2022-10-13T11:37:01.858453Z",
     "shell.execute_reply.started": "2022-10-13T11:37:01.747307Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7052316069602966)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['king', 'woman'], \n",
    "                   negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8a954-4feb-4b5e-894b-f530d9bf96de",
   "metadata": {},
   "source": [
    "Try to find at least three analogies which correctly hold - where \"correctly\" here means that the closest vector corresponds to the word that you as a native speaker think it should."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632d94f-d2c1-4bfb-aaf8-92c2a26c2fab",
   "metadata": {},
   "source": [
    "### Task 3b: Wrong analogies\n",
    "\n",
    "Can you find any analogies which _should_ hold but don't? Why don't they work? Are there any similarities or trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e8721d5-1b49-4f67-89dc-cdea12114722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:51.863501Z",
     "start_time": "2025-03-20T12:42:51.860408Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e672c-9140-49b6-91fa-f8d5364a91f6",
   "metadata": {},
   "source": [
    "### Task 4: Exploring bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f907d-5ecb-4f8a-93b1-e4c19f67e3d0",
   "metadata": {},
   "source": [
    "As we spoke briefly about in the lecture, word embeddings tend to display bias of the kind found in the training data.\n",
    "\n",
    "Using some of the techniques you've worked on above, can you find some clear instances of bias in the word embedding models that you're exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22b2caac-f064-4ee8-8cc4-c81690da786a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:42:53.893671Z",
     "start_time": "2025-03-20T12:42:53.858487Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T11:38:45.594622Z",
     "iopub.status.busy": "2022-10-13T11:38:45.593924Z",
     "iopub.status.idle": "2022-10-13T11:38:45.707115Z",
     "shell.execute_reply": "2022-10-13T11:38:45.705573Z",
     "shell.execute_reply.started": "2022-10-13T11:38:45.594565Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doctors', 0.6494733691215515),\n",
       " ('mother', 0.6095465421676636),\n",
       " ('dentist', 0.5888959169387817),\n",
       " ('birth', 0.5756080150604248),\n",
       " ('grandmother', 0.5664897561073303),\n",
       " ('midwife', 0.5661920309066772),\n",
       " ('nurse', 0.5581483840942383),\n",
       " ('child', 0.5519329905509949),\n",
       " ('daughter', 0.5452523827552795),\n",
       " ('father', 0.5416871905326843)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['doctor', 'woman'], \n",
    "                   negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9179c-307a-4c7b-b3a0-6e0316df4f83",
   "metadata": {},
   "source": [
    "### Task 5: Dimensionality reduction and visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419a1e5-a8be-44df-9334-f03dc427122c",
   "metadata": {},
   "source": [
    "In the following cell, I've (not I Kasper, but Ross. Thanks, Ross!) written a short bit of code which takes a given subset of words and plots them on a simple scatter plot. Remember that the word embeddings are 300d (or 100d here, depending on which model you're using), so we need to perform some kind of dimensionality reduction on the embeddings to get them down to 2D.\n",
    "\n",
    "Here, I'm using a simply PCA algorithm implemented via ```scikit-learn```. An alternative approach might also be to use Singular Value Decomposition or SVD, which works in a similar but ever-so-slightly different way to PCA. You can read more [here](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) and [here](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491) - the maths is bit mind-bending, just FYI.\n",
    "\n",
    "Experiment with plotting certain subsets of words by changing the ```words``` list. How useful do you find these plots? Do they show anything meaningful?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2555a971-2538-416b-b3c6-7c1732893d2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:43:54.144198Z",
     "start_time": "2025-03-20T12:43:52.360411Z"
    },
    "execution": {
     "iopub.execute_input": "2022-10-13T12:10:20.405678Z",
     "iopub.status.busy": "2022-10-13T12:10:20.404976Z",
     "iopub.status.idle": "2022-10-13T12:10:20.570076Z",
     "shell.execute_reply": "2022-10-13T12:10:20.569514Z",
     "shell.execute_reply.started": "2022-10-13T12:10:20.405623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the list of words we want to plot\n",
    "words = [\"man\", \"woman\", \"doctor\", \"nurse\", \"king\", \"queen\", \"boy\", \"girl\"]\n",
    "\n",
    "# an empty list for vectors\n",
    "X = []\n",
    "# get vectors for subset of words\n",
    "for word in words:\n",
    "    X.append(model[word])\n",
    "\n",
    "# Use PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# or try SVD - how are they different?\n",
    "#svd = TruncatedSVD(n_components=2)\n",
    "# fit_transform the initialized PCA model\n",
    "#result = svd.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "# for each word in the list of words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831a495",
   "metadata": {},
   "source": [
    "### Bonus tasks\n",
    "\n",
    "If you run out of things to explore with these embeddings, try some of the following tasks:\n",
    "\n",
    "[Easier]\n",
    "- Write a ```.py``` script which takes the visualization code above which can be run from the command line directly\n",
    "\n",
    "[Medium]\n",
    "- Using what you (might) know about ```matplotlib``` make new plots like those above but cleaner and more informative\n",
    "\n",
    "[Harder]\n",
    "- Word embeddings are numerical vectors for individual tokens. Can you think about how you would create a document-level embedding for, say, a full sentence?\n",
    "    - Try to implement a crude method of creating a sentence embedding from word embeddings in that sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8a60d9c118db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
