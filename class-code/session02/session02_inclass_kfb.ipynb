{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b960cf5d45a879",
   "metadata": {},
   "source": [
    "# Text as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a416f6-f9bf-4ea2-8452-ff1a2332b181",
   "metadata": {},
   "source": [
    "## Segmentation and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e646be-bb26-4621-9f36-894ca7f39e80",
   "metadata": {},
   "source": [
    "In this section we will try to chop up a piece of text into words (tokens) that we can count and analyse."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbf7dce8-6588-4ba8-9a49-675043cd720e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.288790Z",
     "start_time": "2025-05-20T10:24:08.286414Z"
    }
   },
   "source": [
    "example_string = \"This is an example string. It will illustrate how text data is not trivial to work with, even if it is nice and clean.\\n\\nThings like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', can cause trouble for simple approaches of segmentation and tokenization.\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "8a8e4cfa-5320-42b9-a9e1-328fc7e73192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.306636Z",
     "start_time": "2025-05-20T10:24:08.305159Z"
    }
   },
   "source": [
    "print(example_string)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example string. It will illustrate how text data is not trivial to work with, even if it is nice and clean.\n",
      "\n",
      "Things like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', can cause trouble for simple approaches of segmentation and tokenization.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "c4b112fe-f2e4-40bf-a449-65dcd3bdaa87",
   "metadata": {},
   "source": [
    "A naive approach of splitting a text periods for sentences and on whitespace gets us pretty far, but as we shall see, will not get us all the way. Let's try out a few things."
   ]
  },
  {
   "cell_type": "code",
   "id": "146078b2-41d4-4491-9b91-bfc2b8416175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.317909Z",
     "start_time": "2025-05-20T10:24:08.315942Z"
    }
   },
   "source": [
    "example_string.split('.')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example string',\n",
       " ' It will illustrate how text data is not trivial to work with, even if it is nice and clean',\n",
       " '\\n\\nThings like punctuation and abbreviations, e',\n",
       " 'g',\n",
       " \" 'cf\",\n",
       " \"' and 'i\",\n",
       " 'e',\n",
       " \"', can cause trouble for simple approaches of segmentation and tokenization\",\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "fe50c01f-de16-46c8-94ed-851fbb4cad3f",
   "metadata": {},
   "source": [
    "We mostly get complete sentences, but the abbreviations cause trouble. Let's forget sentences for now and focus on individual words. We start by splitting on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "id": "874028b8-7a10-45a3-a5c2-66ceb78a1b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.331569Z",
     "start_time": "2025-05-20T10:24:08.329739Z"
    }
   },
   "source": [
    "example_string.split(' ')[:20]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'illustrate',\n",
       " 'how',\n",
       " 'text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with,',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "a4725a93-8ef8-44dd-a768-48525afb6231",
   "metadata": {},
   "source": [
    "Hmm, the period sticks to the `string.` token and the comma to the `with,` token. Let's try a regex which will split on (most) punctuation and a whitespace by defining a group of punctuation characters that _may_ (see the use of the `?` operator) precede a space."
   ]
  },
  {
   "cell_type": "code",
   "id": "43390ddf-e2bd-43f6-b1f3-1a77dac4fcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.344051Z",
     "start_time": "2025-05-20T10:24:08.342493Z"
    }
   },
   "source": [
    "import re # regular expression module"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c4074b65-9bbb-4c8b-a36f-a7cd20b9ff2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.351511Z",
     "start_time": "2025-05-20T10:24:08.349553Z"
    }
   },
   "source": [
    "pattern = '[.,;?!]? '\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string, )[:30]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " 'It',\n",
       " 'will',\n",
       " 'illustrate',\n",
       " 'how',\n",
       " 'text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean.\\n\\nThings',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "30018170-eddc-42a5-9da4-f58d1c169172",
   "metadata": {},
   "source": [
    "Better. But we have some newlines (`\\n`) causing trouble. We can use the whitespace `\\s` and say that there should be one or more of them by using the `+` operator."
   ]
  },
  {
   "cell_type": "code",
   "id": "d44e9c1f-31c0-4551-9774-6d320f8f241f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.365968Z",
     "start_time": "2025-05-20T10:24:08.363794Z"
    }
   },
   "source": [
    "pattern = r'[.,;?!]?\\s+'\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string)[20:40]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean',\n",
       " 'Things',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\",\n",
       " 'and',\n",
       " \"'i.e.'\",\n",
       " 'can',\n",
       " 'cause',\n",
       " 'trouble',\n",
       " 'for',\n",
       " 'simple',\n",
       " 'approaches',\n",
       " 'of']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "46d9ad97-3aa9-4555-aacd-066b1fd6f368",
   "metadata": {},
   "source": [
    "Hmm, notice that we also lost the final period in `e.g.`, resulting in a truncated `e.g` token. We could handle the different abbreviations in our regex. Also, we may actually want to keep the punctuation as tokens themselves, but probably not the whitespaces since they are not that interesting.\n",
    "\n",
    "You can probably see where this is going. To handle all cases that do not conform to a simple \"split on whitespace\" approach, the regex grows more complex.\n",
    "\n",
    "Instead of reinventing the wheel, we will use tokenizers from the Natural Language Toolkit for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "id": "89175fa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.931846Z",
     "start_time": "2025-05-20T10:24:08.372490Z"
    }
   },
   "source": "!pip install \"nltk<=3.8.1\"",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk<=3.8.1 in /Users/au479461/PycharmProjects/language-analytics/.venv/lib/python3.13/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/au479461/PycharmProjects/language-analytics/.venv/lib/python3.13/site-packages (from nltk<=3.8.1) (8.2.0)\r\n",
      "Requirement already satisfied: joblib in /Users/au479461/PycharmProjects/language-analytics/.venv/lib/python3.13/site-packages (from nltk<=3.8.1) (1.5.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/au479461/PycharmProjects/language-analytics/.venv/lib/python3.13/site-packages (from nltk<=3.8.1) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /Users/au479461/PycharmProjects/language-analytics/.venv/lib/python3.13/site-packages (from nltk<=3.8.1) (4.67.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "d62ab366-83e4-4273-b7e7-f81773cfee62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.940905Z",
     "start_time": "2025-05-20T10:24:08.938284Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import ssl\n",
    "\n",
    "# See: https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt') # underlying model for tokenization"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/au479461/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "1896bd91-7226-47eb-b11a-c39fe5aa53e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.957119Z",
     "start_time": "2025-05-20T10:24:08.948410Z"
    }
   },
   "source": [
    "tokens = word_tokenize(example_string)\n",
    "tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'illustrate',\n",
       " 'how',\n",
       " 'text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " ',',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean',\n",
       " '.',\n",
       " 'Things',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " ',',\n",
       " 'e.g',\n",
       " '.',\n",
       " \"'cf\",\n",
       " '.',\n",
       " \"'\",\n",
       " 'and',\n",
       " \"'\",\n",
       " 'i.e',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'trouble',\n",
       " 'for',\n",
       " 'simple',\n",
       " 'approaches',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'and',\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "666fd2fa-8d01-497c-adf7-4b65684c1858",
   "metadata": {},
   "source": [
    "We can count those tokens and see what makes it to the top."
   ]
  },
  {
   "cell_type": "code",
   "id": "d927387f-28c9-4e5d-985e-c6786e883b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.964919Z",
     "start_time": "2025-05-20T10:24:08.962871Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6),\n",
       " ('and', 4),\n",
       " ('is', 3),\n",
       " (',', 3),\n",
       " (\"'\", 3),\n",
       " ('This', 1),\n",
       " ('an', 1),\n",
       " ('example', 1),\n",
       " ('string', 1),\n",
       " ('It', 1),\n",
       " ('will', 1),\n",
       " ('illustrate', 1),\n",
       " ('how', 1),\n",
       " ('text', 1),\n",
       " ('data', 1),\n",
       " ('not', 1),\n",
       " ('trivial', 1),\n",
       " ('to', 1),\n",
       " ('work', 1),\n",
       " ('with', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "b79f4a87-7b27-4cf2-89be-0033b7ee3180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.977581Z",
     "start_time": "2025-05-20T10:24:08.976468Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f65f1689-ffd3-4fec-8574-39e170b63328",
   "metadata": {},
   "source": [
    "## Loading some text data\n",
    "Let's get some simple text data to work with. It does not matter much exactly what - just that it is plain text data.\n",
    "\n",
    "A good place to start is [Project Gutenberg](https://www.gutenberg.org/) which has free e-books available in plain text files. I have downloaded the top books and stored them in the `data` folder.\n",
    "\n",
    "You can try to find other text sources and load them in."
   ]
  },
  {
   "cell_type": "code",
   "id": "5dd14edd-148c-469a-ac2e-2364798bf05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:08.987751Z",
     "start_time": "2025-05-20T10:24:08.986003Z"
    }
   },
   "source": [
    "from glob import glob\n",
    "\n",
    "for filename in glob('../gutenberg/*.txt'):\n",
    "    print(filename)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "066f5919-8660-453b-bf74-fe4654223486",
   "metadata": {},
   "source": [
    "Let's try to work with the text data from _Moby Dick_. Load the text, and chop it into chapters with a regex."
   ]
  },
  {
   "cell_type": "code",
   "id": "0f3e55d5-cf67-460d-b124-bf926736144d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:44.284611Z",
     "start_time": "2025-05-20T10:24:44.261262Z"
    }
   },
   "source": [
    "chapter_splitter = re.compile(r'\\n{3,}CHAPTER \\d+\\. ')\n",
    "\n",
    "with open('../../data/gutenberg/moby_dick.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    chapters = chapter_splitter.split(raw_text)[1:-1] # removing title, preface etc. up to chapter 1 by slicing\n",
    "\n",
    "for chapter in chapters[:10]:\n",
    "    print(chapter[:100] + ' ...')\n",
    "    print('-'*50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money i ...\n",
      "--------------------------------------------------\n",
      "The Carpet-Bag.\n",
      "\n",
      "I stuffed a shirt or two into my old carpet-bag, tucked it under my\n",
      "arm, and starte ...\n",
      "--------------------------------------------------\n",
      "The Spouter-Inn.\n",
      "\n",
      "Entering that gable-ended Spouter-Inn, you found yourself in a wide,\n",
      "low, straggli ...\n",
      "--------------------------------------------------\n",
      "The Counterpane.\n",
      "\n",
      "Upon waking next morning about daylight, I found Queequeg’s arm thrown\n",
      "over me in  ...\n",
      "--------------------------------------------------\n",
      "Breakfast.\n",
      "\n",
      "I quickly followed suit, and descending into the bar-room accosted the\n",
      "grinning landlord ...\n",
      "--------------------------------------------------\n",
      "The Street.\n",
      "\n",
      "If I had been astonished at first catching a glimpse of so outlandish\n",
      "an individual as  ...\n",
      "--------------------------------------------------\n",
      "The Chapel.\n",
      "\n",
      "In this same New Bedford there stands a Whaleman’s Chapel, and few are\n",
      "the moody fisher ...\n",
      "--------------------------------------------------\n",
      "The Pulpit.\n",
      "\n",
      "I had not been seated very long ere a man of a certain venerable\n",
      "robustness entered; im ...\n",
      "--------------------------------------------------\n",
      "The Sermon.\n",
      "\n",
      "Father Mapple rose, and in a mild voice of unassuming authority ordered\n",
      "the scattered p ...\n",
      "--------------------------------------------------\n",
      "A Bosom Friend.\n",
      "\n",
      "Returning to the Spouter-Inn from the Chapel, I found Queequeg there\n",
      "quite alone; h ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "a693f527-830b-4964-8bbb-a2d6d1edae22",
   "metadata": {},
   "source": [
    "Let's count some words from the first chapter!"
   ]
  },
  {
   "cell_type": "code",
   "id": "32d01b31-1f45-491f-a39e-74bff66391c1",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:45.605938Z",
     "start_time": "2025-05-20T10:24:45.599995Z"
    }
   },
   "source": [
    "first_chapter = chapters[0] # remember zero indexing!\n",
    "tokens = word_tokenize(first_chapter)\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 169),\n",
       " ('the', 121),\n",
       " ('of', 81),\n",
       " ('.', 79),\n",
       " ('a', 68),\n",
       " ('and', 66),\n",
       " ('to', 53),\n",
       " ('in', 46),\n",
       " ('I', 43),\n",
       " ('is', 34),\n",
       " ('that', 31),\n",
       " ('it', 26),\n",
       " ('as', 26),\n",
       " (';', 25),\n",
       " ('me', 24),\n",
       " ('all', 23),\n",
       " ('you', 23),\n",
       " ('?', 18),\n",
       " ('this', 16),\n",
       " ('my', 14)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "ab99d7b6-66ce-4415-bcaa-750dde6ff744",
   "metadata": {},
   "source": [
    "This is a bit more interesting than the two or one word counts from the example sentence above. But it still does not tell us much since  the top ranking words are just punctuation and function words like _the_ and _of_. Let's do some filtering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fbc0-2eb1-4234-b9c8-a5c6b2805c09",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba113403-f4ab-47d5-af57-354d59619eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:46.768340Z",
     "start_time": "2025-05-20T10:24:46.596662Z"
    }
   },
   "source": [
    "from string import punctuation\n",
    "from nltk import corpus\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/au479461/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "a8a08692-87eb-447c-94bd-f8c06e854245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:46.818124Z",
     "start_time": "2025-05-20T10:24:46.814255Z"
    }
   },
   "source": [
    "punctuation_set = set(punctuation)\n",
    "stopwords_set = set(corpus.stopwords.words('english'))"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "de5c7290-c2e9-4414-bc18-759d49e9fc2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:47.011965Z",
     "start_time": "2025-05-20T10:24:47.010093Z"
    }
   },
   "source": [
    "# short-hand way with list comprehension:\n",
    "# filtered_tokens = [token for token in tokens if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_tokens.append(token)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "7efc3fb5-1e40-4598-8aba-57e90fe7ea75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:47.265010Z",
     "start_time": "2025-05-20T10:24:47.262622Z"
    }
   },
   "source": [
    "filtered_counter = Counter(filtered_tokens)\n",
    "filtered_counter.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 43),\n",
       " ('sea', 10),\n",
       " ('one', 10),\n",
       " ('go', 10),\n",
       " ('upon', 9),\n",
       " ('But', 8),\n",
       " ('part', 7),\n",
       " ('’', 7),\n",
       " ('And', 7),\n",
       " ('see', 6),\n",
       " ('It', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('old', 6),\n",
       " ('take', 5),\n",
       " ('What', 5),\n",
       " ('ever', 5)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "e16bdb10-f978-49b3-8fd9-19135d671a9f",
   "metadata": {},
   "source": [
    "Better. But this reveals an issue that we need to fix: some of the stopwords are not removed because they begin with a capital letter, e.g. _But_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6897b-dffe-4385-9e9e-d1e95e741455",
   "metadata": {},
   "source": [
    "We can also make other observations.\n",
    "\n",
    "The first is that capitalized and non-capitalized versions of a word (e.g. _Tell_ vs _tell_) are counted as two different words. Depending on our analysis, we may want to count them as one.\n",
    "\n",
    "The second is that inflected and non-inflected versions of a word (e.g. _passenger_ vs _passengers_) are counted as different words. Again, depending on our analysis, we may want to count them as one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca5554-8b74-4b9a-90c5-b6b3443825c3",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817c75f-435f-4313-b01f-bd44ccddf621",
   "metadata": {},
   "source": [
    "Normalization is about handling differences. This includes things like:\n",
    "1. Capitalization, e.g. _Tell_ vs _tell_.\n",
    "2. UK and US spelling, e.g. _colour_ vs. _color_. (We will skip that here; it is mostly relevant across different texts)\n",
    "3. Inflection, e.g. _passenger_ vs _passengers_.\n",
    "\n",
    "Be mindful that each such a normalization process eliminate potentially meaningful information. That is, there are reasons for capitalization; consider _apple_ vs _Apple_ or _bill_ vs _Bill_, and plural inflection or past tense are there for a reason.\n",
    "\n",
    "For this exercise we will do decapitalization and stemming."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb392e1d-8625-421d-939d-f148aad20a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:48.714567Z",
     "start_time": "2025-05-20T10:24:48.711876Z"
    }
   },
   "source": [
    "# short-hand way with list comprehension\n",
    "# lowercase_tokens = [token.lower() for token in tokens]\n",
    "# filtered_lowercase_tokens = [token for token in lowercase_tokens\n",
    "#                              if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "lowercase_tokens = []\n",
    "for token in tokens:\n",
    "    lowercase_tokens.append(token.lower())\n",
    "\n",
    "filtered_lowercase_tokens = []\n",
    "for token in lowercase_tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_lowercase_tokens.append(token)\n",
    "\n",
    "filtered_lowercase_tokens[:50]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loomings',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'years',\n",
       " 'ago—never',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely—having',\n",
       " 'little',\n",
       " 'money',\n",
       " 'purse',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'little',\n",
       " 'see',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'driving',\n",
       " 'spleen',\n",
       " 'regulating',\n",
       " 'circulation',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'growing',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenever',\n",
       " 'damp',\n",
       " 'drizzly',\n",
       " 'november',\n",
       " 'soul',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'involuntarily',\n",
       " 'pausing',\n",
       " 'coffin',\n",
       " 'warehouses',\n",
       " 'bringing',\n",
       " 'rear',\n",
       " 'every',\n",
       " 'funeral',\n",
       " 'meet',\n",
       " 'especially']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "f79217ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.319303Z",
     "start_time": "2025-05-20T10:24:49.316889Z"
    }
   },
   "source": [
    "filtered_lowercase_counter = Counter(filtered_lowercase_tokens)\n",
    "filtered_lowercase_counter.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 12),\n",
       " ('one', 11),\n",
       " ('sea', 10),\n",
       " ('upon', 9),\n",
       " ('part', 7),\n",
       " ('’', 7),\n",
       " ('see', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('take', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyage', 6),\n",
       " ('old', 6),\n",
       " ('whenever', 5),\n",
       " ('ever', 5),\n",
       " ('sailor', 5),\n",
       " ('whaling', 5),\n",
       " ('little', 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "c7696bca-c7e7-48f3-aa6d-c8cd56b512cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.593854Z",
     "start_time": "2025-05-20T10:24:49.584903Z"
    }
   },
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# short-hand way with list comprehension\n",
    "# stemmed_filtered_lowercase_tokens = [stemmer.stem(token) for token in filtered_lowercase_tokens]\n",
    "\n",
    "stemmed_filtered_lowercase_tokens = []\n",
    "for token in filtered_lowercase_tokens:\n",
    "    stemmed = stemmer.stem(token)\n",
    "    stemmed_filtered_lowercase_tokens.append(stemmed)\n",
    "\n",
    "stemmed_filtered_lowercase_tokens[:50]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loom',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'year',\n",
       " 'ago—nev',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely—hav',\n",
       " 'littl',\n",
       " 'money',\n",
       " 'purs',\n",
       " 'noth',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'littl',\n",
       " 'see',\n",
       " 'wateri',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'drive',\n",
       " 'spleen',\n",
       " 'regul',\n",
       " 'circul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'grow',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenev',\n",
       " 'damp',\n",
       " 'drizzli',\n",
       " 'novemb',\n",
       " 'soul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'involuntarili',\n",
       " 'paus',\n",
       " 'coffin',\n",
       " 'warehous',\n",
       " 'bring',\n",
       " 'rear',\n",
       " 'everi',\n",
       " 'funer',\n",
       " 'meet',\n",
       " 'especi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "8291d648-4bfe-49a1-8441-eeaf94ba8f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:24:49.915071Z",
     "start_time": "2025-05-20T10:24:49.912519Z"
    }
   },
   "source": [
    "counter = Counter(stemmed_filtered_lowercase_tokens)\n",
    "counter.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 15),\n",
       " ('sea', 12),\n",
       " ('one', 11),\n",
       " ('part', 10),\n",
       " ('upon', 9),\n",
       " ('whale', 8),\n",
       " ('get', 7),\n",
       " ('’', 7),\n",
       " ('take', 7),\n",
       " ('passeng', 7),\n",
       " ('see', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyag', 6),\n",
       " ('old', 6),\n",
       " ('thing', 6),\n",
       " ('sailor', 6),\n",
       " ('whenev', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "7fabd93d-8c83-4abf-9b4a-5626a7cbdc44",
   "metadata": {},
   "source": [
    "### A note on stemming\n",
    "Stemming can be quite aggressive in how much it removes of a word, e.g. _passeng_ from _passenger_ and _passengers_. An alternative is lemmatization which gives the dictionary lookup form of a word, but it requires POS-tags on the tokens to work properly. That is, _passengers_ and _passenger_ would both become _passenger_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
