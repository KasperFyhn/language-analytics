{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b960cf5d45a879",
   "metadata": {},
   "source": [
    "# Text as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a416f6-f9bf-4ea2-8452-ff1a2332b181",
   "metadata": {},
   "source": [
    "## Segmentation and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e646be-bb26-4621-9f36-894ca7f39e80",
   "metadata": {},
   "source": [
    "In this section we will try to chop up a piece of text into words (tokens) that we can count and analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7dce8-6588-4ba8-9a49-675043cd720e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:58:31.302146Z",
     "start_time": "2025-02-17T07:58:31.298333Z"
    }
   },
   "outputs": [],
   "source": [
    "example_string = \"This is an example string. It will illustrate how text data is not trivial to work with, even if it is nice and clean.\\n\\nThings like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', can cause trouble for simple approaches of segmentation and tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e4cfa-5320-42b9-a9e1-328fc7e73192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:58:32.039604Z",
     "start_time": "2025-02-17T07:58:32.036290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example string. It will illustrate how text data is not trivial to work with, even if it is nice and clean.\n",
      "\n",
      "Things like punctuation and abbreviations, e.g. 'cf.' and 'i.e.', can cause trouble for simple approaches of segmentation and tokenization.\n"
     ]
    }
   ],
   "source": [
    "print(example_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b112fe-f2e4-40bf-a449-65dcd3bdaa87",
   "metadata": {},
   "source": [
    "A naive approach of splitting a text periods for sentences and on whitespace gets us pretty far, but as we shall see, will not get us all the way. Let's try out a few things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146078b2-41d4-4491-9b91-bfc2b8416175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:58:33.731675Z",
     "start_time": "2025-02-17T07:58:33.725847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is an example string',\n",
       " ' It will illustrate how text data is not trivial to work with, even if it is nice and clean',\n",
       " '\\n\\nThings like punctuation and abbreviations, e',\n",
       " 'g',\n",
       " \" 'cf\",\n",
       " \"' and 'i\",\n",
       " 'e',\n",
       " \"', can cause trouble for simple approaches of segmentation and tokenization\",\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50c01f-de16-46c8-94ed-851fbb4cad3f",
   "metadata": {},
   "source": [
    "We mostly get complete sentences, but the abbreviations cause trouble. Let's forget sentences for now and focus on individual words. We start by splitting on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874028b8-7a10-45a3-a5c2-66ceb78a1b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:58:35.149939Z",
     "start_time": "2025-02-17T07:58:35.144009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'illustrate',\n",
       " 'how',\n",
       " 'text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with,',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string.split(' ')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4725a93-8ef8-44dd-a768-48525afb6231",
   "metadata": {},
   "source": [
    "Hmm, the period sticks to the `string.` token and the comma to the `with,` token. Let's try a regex which will split on (most) punctuation and a whitespace by defining a group of punctuation characters that _may_ (see the use of the `?` operator) precede a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43390ddf-e2bd-43f6-b1f3-1a77dac4fcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:58:38.526635Z",
     "start_time": "2025-02-17T07:58:38.517866Z"
    }
   },
   "outputs": [],
   "source": [
    "import re # regular expression module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4074b65-9bbb-4c8b-a36f-a7cd20b9ff2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:59:54.372158Z",
     "start_time": "2025-02-17T07:59:54.367912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'string',\n",
       " 'It',\n",
       " 'will',\n",
       " 'illustrate',\n",
       " 'how',\n",
       " 'text',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'even',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean.\\n\\nThings',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = '[.,;?!]? '\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string, )[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30018170-eddc-42a5-9da4-f58d1c169172",
   "metadata": {},
   "source": [
    "Better. But we have some newlines (`\\n`) causing trouble. We can use the whitespace `\\s` and say that there should be one or more of them by using the `+` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44e9c1f-31c0-4551-9774-6d320f8f241f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T07:59:56.991124Z",
     "start_time": "2025-02-17T07:59:56.986997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'clean',\n",
       " 'Things',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'abbreviations',\n",
       " 'e.g',\n",
       " \"'cf.'\",\n",
       " 'and',\n",
       " \"'i.e.'\",\n",
       " 'can',\n",
       " 'cause',\n",
       " 'trouble',\n",
       " 'for',\n",
       " 'simple',\n",
       " 'approaches',\n",
       " 'of']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'[.,;?!]?\\s+'\n",
    "regex = re.compile(pattern)\n",
    "regex.split(example_string)[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9ad97-3aa9-4555-aacd-066b1fd6f368",
   "metadata": {},
   "source": [
    "Hmm, notice that we also lost the final period in `e.g.`, resulting in a truncated `e.g` token. We could handle the different abbreviations in our regex. Also, we may actually want to keep the punctuation as tokens themselves, but probably not the whitespaces since they are not that interesting.\n",
    "\n",
    "You can probably see where this is going. To handle all cases that do not conform to a simple \"split on whitespace\" approach, the regex grows more complex.\n",
    "\n",
    "Instead of reinventing the wheel, we will use tokenizers from the Natural Language Toolkit for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89175fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62ab366-83e4-4273-b7e7-f81773cfee62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:00:27.611217Z",
     "start_time": "2025-02-17T08:00:25.995529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt') # underlying model for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1896bd91-7226-47eb-b11a-c39fe5aa53e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:00:31.894521Z",
     "start_time": "2025-02-17T08:00:31.872526Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ucloud/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokens\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ucloud/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(example_string)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fd2fa-8d01-497c-adf7-4b65684c1858",
   "metadata": {},
   "source": [
    "We can count those tokens and see what makes it to the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927387f-28c9-4e5d-985e-c6786e883b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:00:41.469128Z",
     "start_time": "2025-02-17T08:00:41.464210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6),\n",
       " ('and', 4),\n",
       " ('is', 3),\n",
       " (',', 3),\n",
       " (\"'\", 3),\n",
       " ('This', 1),\n",
       " ('an', 1),\n",
       " ('example', 1),\n",
       " ('string', 1),\n",
       " ('It', 1),\n",
       " ('will', 1),\n",
       " ('illustrate', 1),\n",
       " ('how', 1),\n",
       " ('text', 1),\n",
       " ('data', 1),\n",
       " ('not', 1),\n",
       " ('trivial', 1),\n",
       " ('to', 1),\n",
       " ('work', 1),\n",
       " ('with', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f4a87-7b27-4cf2-89be-0033b7ee3180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:38:10.461895Z",
     "start_time": "2024-06-26T07:38:10.459886Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65f1689-ffd3-4fec-8574-39e170b63328",
   "metadata": {},
   "source": [
    "## Loading some text data\n",
    "Let's get some simple text data to work with. It does not matter much exactly what - just that it is plain text data.\n",
    "\n",
    "A good place to start is [Project Gutenberg](https://www.gutenberg.org/) which has free e-books available in plain text files. I have downloaded the top books and stored them in the `data` folder.\n",
    "\n",
    "You can try to find other text sources and load them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd14edd-148c-469a-ac2e-2364798bf05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:02:07.656Z",
     "start_time": "2025-02-17T08:02:07.652799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../gutenberg/middlemarch.txt\n",
      "../gutenberg/moby_dick.txt\n",
      "../gutenberg/pride_and_prejudice.txt\n",
      "../gutenberg/a_room_with_a_view.txt\n",
      "../gutenberg/romeo_and_juliet.txt\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "for filename in glob('../gutenberg/*.txt'):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f5919-8660-453b-bf74-fe4654223486",
   "metadata": {},
   "source": [
    "Let's try to work with the text data from _Moby Dick_. Load the text, and chop it into chapters with a regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e55d5-cf67-460d-b124-bf926736144d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:02:09.858163Z",
     "start_time": "2025-02-17T08:02:09.838966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago—never mind how long precisely—having\n",
      "little or no money i ...\n",
      "--------------------------------------------------\n",
      "The Carpet-Bag.\n",
      "\n",
      "I stuffed a shirt or two into my old carpet-bag, tucked it under my\n",
      "arm, and starte ...\n",
      "--------------------------------------------------\n",
      "The Spouter-Inn.\n",
      "\n",
      "Entering that gable-ended Spouter-Inn, you found yourself in a wide,\n",
      "low, straggli ...\n",
      "--------------------------------------------------\n",
      "The Counterpane.\n",
      "\n",
      "Upon waking next morning about daylight, I found Queequeg’s arm thrown\n",
      "over me in  ...\n",
      "--------------------------------------------------\n",
      "Breakfast.\n",
      "\n",
      "I quickly followed suit, and descending into the bar-room accosted the\n",
      "grinning landlord ...\n",
      "--------------------------------------------------\n",
      "The Street.\n",
      "\n",
      "If I had been astonished at first catching a glimpse of so outlandish\n",
      "an individual as  ...\n",
      "--------------------------------------------------\n",
      "The Chapel.\n",
      "\n",
      "In this same New Bedford there stands a Whaleman’s Chapel, and few are\n",
      "the moody fisher ...\n",
      "--------------------------------------------------\n",
      "The Pulpit.\n",
      "\n",
      "I had not been seated very long ere a man of a certain venerable\n",
      "robustness entered; im ...\n",
      "--------------------------------------------------\n",
      "The Sermon.\n",
      "\n",
      "Father Mapple rose, and in a mild voice of unassuming authority ordered\n",
      "the scattered p ...\n",
      "--------------------------------------------------\n",
      "A Bosom Friend.\n",
      "\n",
      "Returning to the Spouter-Inn from the Chapel, I found Queequeg there\n",
      "quite alone; h ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chapter_splitter = re.compile(r'\\n{3,}CHAPTER \\d+\\. ')\n",
    "\n",
    "with open('../gutenberg/moby_dick.txt') as f:\n",
    "    raw_text = f.read()\n",
    "    chapters = chapter_splitter.split(raw_text)[1:-1] # removing title, preface etc. up to chapter 1 by slicing\n",
    "\n",
    "for chapter in chapters[:10]:\n",
    "    print(chapter[:100] + ' ...')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693f527-830b-4964-8bbb-a2d6d1edae22",
   "metadata": {},
   "source": [
    "Let's count some words from the first chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d01b31-1f45-491f-a39e-74bff66391c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:02:25.594781Z",
     "start_time": "2025-02-17T08:02:25.579896Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 169),\n",
       " ('the', 121),\n",
       " ('of', 81),\n",
       " ('.', 79),\n",
       " ('a', 68),\n",
       " ('and', 66),\n",
       " ('to', 53),\n",
       " ('in', 46),\n",
       " ('I', 43),\n",
       " ('is', 34),\n",
       " ('that', 31),\n",
       " ('it', 26),\n",
       " ('as', 26),\n",
       " (';', 25),\n",
       " ('me', 24),\n",
       " ('all', 23),\n",
       " ('you', 23),\n",
       " ('?', 18),\n",
       " ('this', 16),\n",
       " ('my', 14)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chapter = chapters[0] # remember zero indexing!\n",
    "tokens = word_tokenize(first_chapter)\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99d7b6-66ce-4415-bcaa-750dde6ff744",
   "metadata": {},
   "source": [
    "This is a bit more interesting than the two or one word counts from the example sentence above. But it still does not tell us much since  the top ranking words are just punctuation and function words like _the_ and _of_. Let's do some filtering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fbc0-2eb1-4234-b9c8-a5c6b2805c09",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba113403-f4ab-47d5-af57-354d59619eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:02:50.267412Z",
     "start_time": "2025-02-17T08:02:50.261449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk import corpus\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a08692-87eb-447c-94bd-f8c06e854245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:02:53.907646Z",
     "start_time": "2025-02-17T08:02:53.903614Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuation_set = set(punctuation)\n",
    "stopwords_set = set(corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c7290-c2e9-4414-bc18-759d49e9fc2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:03:41.142717Z",
     "start_time": "2025-02-17T08:03:41.139683Z"
    }
   },
   "outputs": [],
   "source": [
    "# short-hand way with list comprehension:\n",
    "# filtered_tokens = [token for token in tokens if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc3fb5-1e40-4598-8aba-57e90fe7ea75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:03:42.325553Z",
     "start_time": "2025-02-17T08:03:42.320987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 43),\n",
       " ('sea', 10),\n",
       " ('one', 10),\n",
       " ('go', 10),\n",
       " ('upon', 9),\n",
       " ('But', 8),\n",
       " ('part', 7),\n",
       " ('’', 7),\n",
       " ('And', 7),\n",
       " ('see', 6),\n",
       " ('It', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('old', 6),\n",
       " ('take', 5),\n",
       " ('What', 5),\n",
       " ('ever', 5)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_counter = Counter(filtered_tokens)\n",
    "filtered_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bdb10-f978-49b3-8fd9-19135d671a9f",
   "metadata": {},
   "source": [
    "Better. But this reveals an issue that we need to fix: some of the stopwords are not removed because they begin with a capital letter, e.g. _But_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6897b-dffe-4385-9e9e-d1e95e741455",
   "metadata": {},
   "source": [
    "We can also make other observations.\n",
    "\n",
    "The first is that capitalized and non-capitalized versions of a word (e.g. _Tell_ vs _tell_) are counted as two different words. Depending on our analysis, we may want to count them as one.\n",
    "\n",
    "The second is that inflected and non-inflected versions of a word (e.g. _passenger_ vs _passengers_) are counted as different words. Again, depending on our analysis, we may want to count them as one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca5554-8b74-4b9a-90c5-b6b3443825c3",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817c75f-435f-4313-b01f-bd44ccddf621",
   "metadata": {},
   "source": [
    "Normalization is about handling differences. This includes things like:\n",
    "1. Capitalization, e.g. _Tell_ vs _tell_.\n",
    "2. UK and US spelling, e.g. _colour_ vs. _color_. (We will skip that here; it is mostly relevant across different texts)\n",
    "3. Inflection, e.g. _passenger_ vs _passengers_.\n",
    "\n",
    "Be mindful that each such a normalization process eliminate potentially meaningful information. That is, there are reasons for capitalization; consider _apple_ vs _Apple_ or _bill_ vs _Bill_, and plural inflection or past tense are there for a reason.\n",
    "\n",
    "For this exercise we will do decapitalization and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb392e1d-8625-421d-939d-f148aad20a58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:05:19.845968Z",
     "start_time": "2025-02-17T08:05:19.840273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loomings',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'years',\n",
       " 'ago—never',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely—having',\n",
       " 'little',\n",
       " 'money',\n",
       " 'purse',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'little',\n",
       " 'see',\n",
       " 'watery',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'driving',\n",
       " 'spleen',\n",
       " 'regulating',\n",
       " 'circulation',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'growing',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenever',\n",
       " 'damp',\n",
       " 'drizzly',\n",
       " 'november',\n",
       " 'soul',\n",
       " 'whenever',\n",
       " 'find',\n",
       " 'involuntarily',\n",
       " 'pausing',\n",
       " 'coffin',\n",
       " 'warehouses',\n",
       " 'bringing',\n",
       " 'rear',\n",
       " 'every',\n",
       " 'funeral',\n",
       " 'meet',\n",
       " 'especially']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short-hand way with list comprehension\n",
    "# lowercase_tokens = [token.lower() for token in tokens]\n",
    "# filtered_lowercase_tokens = [token for token in lowercase_tokens\n",
    "#                              if token not in punctuation_set and token not in stopwords_set]\n",
    "\n",
    "lowercase_tokens = []\n",
    "for token in tokens:\n",
    "    lowercase_tokens.append(token.lower())\n",
    "\n",
    "filtered_lowercase_tokens = []\n",
    "for token in lowercase_tokens:\n",
    "    if token not in punctuation_set and token not in stopwords_set:\n",
    "        filtered_lowercase_tokens.append(token)\n",
    "\n",
    "filtered_lowercase_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79217ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 12),\n",
       " ('one', 11),\n",
       " ('sea', 10),\n",
       " ('upon', 9),\n",
       " ('part', 7),\n",
       " ('’', 7),\n",
       " ('see', 6),\n",
       " ('get', 6),\n",
       " ('time', 6),\n",
       " ('take', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyage', 6),\n",
       " ('old', 6),\n",
       " ('whenever', 5),\n",
       " ('ever', 5),\n",
       " ('sailor', 5),\n",
       " ('whaling', 5),\n",
       " ('little', 4)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lowercase_counter = Counter(filtered_lowercase_tokens)\n",
    "filtered_lowercase_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7696bca-c7e7-48f3-aa6d-c8cd56b512cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:06:18.978421Z",
     "start_time": "2025-02-17T08:06:18.962555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loom',\n",
       " 'call',\n",
       " 'ishmael',\n",
       " 'year',\n",
       " 'ago—nev',\n",
       " 'mind',\n",
       " 'long',\n",
       " 'precisely—hav',\n",
       " 'littl',\n",
       " 'money',\n",
       " 'purs',\n",
       " 'noth',\n",
       " 'particular',\n",
       " 'interest',\n",
       " 'shore',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'sail',\n",
       " 'littl',\n",
       " 'see',\n",
       " 'wateri',\n",
       " 'part',\n",
       " 'world',\n",
       " 'way',\n",
       " 'drive',\n",
       " 'spleen',\n",
       " 'regul',\n",
       " 'circul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'grow',\n",
       " 'grim',\n",
       " 'mouth',\n",
       " 'whenev',\n",
       " 'damp',\n",
       " 'drizzli',\n",
       " 'novemb',\n",
       " 'soul',\n",
       " 'whenev',\n",
       " 'find',\n",
       " 'involuntarili',\n",
       " 'paus',\n",
       " 'coffin',\n",
       " 'warehous',\n",
       " 'bring',\n",
       " 'rear',\n",
       " 'everi',\n",
       " 'funer',\n",
       " 'meet',\n",
       " 'especi']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# short-hand way with list comprehension\n",
    "# stemmed_filtered_lowercase_tokens = [stemmer.stem(token) for token in filtered_lowercase_tokens]\n",
    "\n",
    "stemmed_filtered_lowercase_tokens = []\n",
    "for token in filtered_lowercase_tokens:\n",
    "    stemmed = stemmer.stem(token)\n",
    "    stemmed_filtered_lowercase_tokens.append(stemmed)\n",
    "\n",
    "stemmed_filtered_lowercase_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291d648-4bfe-49a1-8441-eeaf94ba8f53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T08:06:27.761803Z",
     "start_time": "2025-02-17T08:06:27.757261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go', 15),\n",
       " ('sea', 12),\n",
       " ('one', 11),\n",
       " ('part', 10),\n",
       " ('upon', 9),\n",
       " ('whale', 8),\n",
       " ('get', 7),\n",
       " ('’', 7),\n",
       " ('take', 7),\n",
       " ('passeng', 7),\n",
       " ('see', 6),\n",
       " ('time', 6),\n",
       " ('land', 6),\n",
       " ('like', 6),\n",
       " ('water', 6),\n",
       " ('voyag', 6),\n",
       " ('old', 6),\n",
       " ('thing', 6),\n",
       " ('sailor', 6),\n",
       " ('whenev', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(stemmed_filtered_lowercase_tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabd93d-8c83-4abf-9b4a-5626a7cbdc44",
   "metadata": {},
   "source": [
    "### A note on stemming\n",
    "Stemming can be quite aggressive in how much it removes of a word, e.g. _passeng_ from _passenger_ and _passengers_. An alternative is lemmatization which gives the dictionary lookup form of a word, but it requires POS-tags on the tokens to work properly. That is, _passengers_ and _passenger_ would both become _passenger_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436553d8-70ce-4833-afbf-20c837890462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
