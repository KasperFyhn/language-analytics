{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative language models for zero-shot or few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many modern NLP solutions (and software more generally; you know, sprinkle that magic AI dust ...) include the use of LLMs for performing various tasks. Unless an LLM is run on a powerful machine, this often involves sending requests to an LLM service, kind of like when your browser communicates with a server. This can actually be done from the command-line with the `curl` tool, as shown below. This is essentially what we want to do in Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:28:41.266171Z",
     "start_time": "2025-04-30T12:28:26.055734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:15.988960481Z\",\"response\":\"A\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.000652397Z\",\"response\":\" reference\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.010211574Z\",\"response\":\" to\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.0177438Z\",\"response\":\" Pink\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.025058983Z\",\"response\":\" Floyd\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.032516523Z\",\"response\":\"'s\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.039915692Z\",\"response\":\" iconic\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.047482053Z\",\"response\":\" song\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.054696159Z\",\"response\":\" \\\"\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.062314684Z\",\"response\":\"Comfort\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.069500834Z\",\"response\":\"ably\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.076949995Z\",\"response\":\" N\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.084613659Z\",\"response\":\"umb\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.091983646Z\",\"response\":\"\\\"\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.099522129Z\",\"response\":\"!\\n\\n\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.107188333Z\",\"response\":\"The\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.114689821Z\",\"response\":\" phrase\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.122357958Z\",\"response\":\" \\\"\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.129920308Z\",\"response\":\"is\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.137894184Z\",\"response\":\" there\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-05-05T11:30:16.137927252Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"length\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,3957,1070,21739,704,1070,30,128009,128006,78191,128007,271,32,5905,311,27211,46899,596,27373,5609,330,95417,2915,452,3635,1,2268,791,17571,330,285,1070],\"total_duration\":4042552080,\"load_duration\":3726979101,\"prompt_eval_count\":31,\"prompt_eval_duration\":160000000,\"eval_count\":20,\"eval_duration\":152000000}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This will only work if there is a server running with this public URL on UCloud\n",
    "! curl https://app-language-analytics-llm.cloud.aau.dk/api/generate -d '{\"model\": \"llama3.2:3b\", \"prompt\": \"Is there anybody out there?\", \"stream\": true, \"options\": {\"num_predict\": 20}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs (Application Programming Interface_) and web services\n",
    "\n",
    "First a metaphor (thanks to `u/berael` on Reddit for the inspiration): An API is a menu in a restaurant. If someone wants give you access to their food, but not their kitchen, they give you a menu. Now you can tell them what you want and get it without seeing how it is made.\n",
    "\n",
    "If someone wants to give you access to their program, but not their code, they give you an API. Thus, an API gives you access to the functionality of an application without showing you the implementation and inner logic.\n",
    "\n",
    "In fact, we have been working with many things that can be considered having an API. For instance, a lot is going on under the hood in `SpaCy` or `transformers`, and we can access that functionality via their API.\n",
    "\n",
    "When I say _LLM service_, it is because it is typically an API exposed and used over a network via HTTP/HTTPS requests. If you open the _Inspect_ pane in your browser (ctrl+shift+i in Chrome) and open the tab _Network_, you will see that your browser, as the _client_, sends many (often hundreds) requests to web _servers_ to get the content that you see in your browser. In a similar fashion, we can send requests (for data, broadly speaking) in our Python code to a server . We then use the data that we get back for our own purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible APIs in Python\n",
    "Due to the wide usage, a standard quickly arose from early movers: OpenAI. Therefore, communication with an LLM service is often done with a so-called \"OpenAI compatible\" API. This has many nice consequences. For instance, one really only has to learn how to do it one way, and it means that switching from one model (or provider) to another requires little more than switching out a few lines. In practice, it means that we can use the `openai` library, but just point it elsewhere in the requests. We then point the _client_ to an [ollama](https://ollama.com/) server (which is a framework for running local LLMs).\n",
    "\n",
    "For this exercise, I (Kasper) will get servers up and running on UCloud. You can also try to install and run ollama on your own machine, in which case you should use the `http://localhost:11434/v1` endpoint, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:18:32.409806Z",
     "start_time": "2025-05-02T06:18:31.457253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/ucloud/.local/lib/python3.12/site-packages (1.77.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ucloud/.local/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ucloud/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/ucloud/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ucloud/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ucloud/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ucloud/.local/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ucloud/.local/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ucloud/.local/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:38:53.283983Z",
     "start_time": "2025-04-30T12:38:53.281090Z"
    }
   },
   "outputs": [],
   "source": [
    "local_endpoint = 'http://localhost:11434/v1'\n",
    "ucloud_endpoint_1 = 'https://app-language-analytics-llm.cloud.aau.dk/v1'\n",
    "ucloud_endpoint_2 = 'https://app-language-analytics-llm2.cloud.aau.dk/v1'\n",
    "ucloud_endpoint_3 = 'https://app-language-analytics-llm3.cloud.aau.dk/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:29:01.384555Z",
     "start_time": "2025-04-30T12:29:01.381317Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_endpoint = ucloud_endpoint_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:22:12.794245Z",
     "start_time": "2025-05-02T06:22:11.296707Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=llm_endpoint,\n",
    "    api_key='ollama',  # required, but unused; for OpenAI or similar, this the secret access key that tells the server who you are\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I make proper ramen?\"},\n",
    "    ],\n",
    "    max_tokens=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:33:37.421463Z",
     "start_time": "2025-04-30T12:33:37.417452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Making proper ramen is an art that requires attention to detail, quality ingredients,'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available models for this exercise will be:\n",
    "- llama3.2:3b\n",
    "- llama3.1:8b\n",
    "- phi4\n",
    "- deepseek-r1:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The old way\n",
    "Notice that the arguments and the endpoint are slightly different from what I used in the `curl` command further above. The one in Python is considered the modern one, where it is laid out as a chat between `user` and `assistant`. The other is considered outdated, but still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:37:46.202533Z",
     "start_time": "2025-04-30T12:37:32.001352Z"
    }
   },
   "outputs": [],
   "source": [
    "response = client.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    prompt=\"How do I make proper ramen?\",\n",
    "    max_tokens=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:37:53.546930Z",
     "start_time": "2025-04-30T12:37:53.543039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='cmpl-324', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='The Los Angeles Dodgers won the World Series in ')], created=1746016666, model='llama3.2:3b', object='text_completion', system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=10, prompt_tokens=35, total_tokens=45, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Our goal is to use the knowledge of language that instruction-tuned language models like Llama (from Meta) or Phi (from Microsoft) have already acquired during training and to use that knowledge in different domains *without any further fine-tuning*. This is called *zero-shot* learning. If we provide one or more examples as part of the context, it is referred to as *few-shot* learning.\n",
    "\n",
    "In order for zero-shot learning to be successful, our prompts need to be carefully designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "prompt = \"classify the following text as positive or negative: I absolutely hated this movie\"\n",
    "\n",
    "# translation\n",
    "#prompt = \"translate from English to French: how old are you?\"\n",
    "\n",
    "# question answering\n",
    "#prompt = \"answer the following question: how is cheese made?\"\n",
    "\n",
    "# named entity recognition\n",
    "#prompt = \"find all location entities in this text: Ross comes from Scotland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:44:01.619652Z",
     "start_time": "2025-04-30T12:43:51.582815Z"
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a data annotator. Follow the instructions and respond with only one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"classify the following text as 'positive' or 'negative':\\n\\nI absolutely hated this movie\"},\n",
    "    ],\n",
    "    max_tokens=1  # we really just need one!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:44:27.021987Z",
     "start_time": "2025-04-30T12:44:27.018756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this a bit more cleverly by using a single prompt plus a F-string. This means we could, for example, write functions for specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:45:54.012882Z",
     "start_time": "2025-04-30T12:45:54.009332Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifier(input_text: str) -> str:\n",
    "    prompt = f\"classify the following text as positive or negative: {input_text}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.2:3b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a data annotator. Follow the instructions and respond with only one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=1  # we really just need one!\n",
    "    )\n",
    "    return response.choices[0].message.content.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:45:55.899345Z",
     "start_time": "2025-04-30T12:45:54.739303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I absolutely hated this movie!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Look through previous notebooks, exercises, and datasets from Language Analytics so far this semester. In small groups, try using either a local model, the UCloud endpoints or your favorite chatbot (ChatGPT, Claude, Gemini, Mistral, etc.). Try to solve those problems using generative language models. That would mean, for example:\n",
    "    - Grammatical analysis\n",
    "    - Named entity recognition/extraction\n",
    "    - Classification\n",
    "    - Topic modelling\n",
    "2. The API has a lot of options, e.g. sampling strategies, ways to control generation or do integrate \"streaming\" (one token at a time). It surpasses the scope of this class to go through all of that, but now you may know where to look. If you are up for it, try tweaking the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
